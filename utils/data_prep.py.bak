import pandas as pd
import json
from tqdm.auto import tqdm
from utils.openai_logic import create_embeddings
import os, sys
from .auth import get_confluence_client
import re
import urllib.parse
from bs4 import BeautifulSoup
import nltk
import ast

# Try to download NLTK data if not already present
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

def semantic_chunking(text, max_chunk_size=1000, overlap=100):
    """
    Split text into semantic chunks that respect content boundaries.
    
    Args:
        text: The text to split into chunks
        max_chunk_size: Maximum size of each chunk (in characters)
        overlap: Number of characters to overlap between chunks
        
    Returns:
        List of chunks with metadata about their position in the document
    """
    # Check if this is HTML content
    is_html = bool(re.search(r'<\w+[^>]*>', text))
    
    if is_html:
        return semantic_chunking_html(text, max_chunk_size, overlap)
    else:
        return semantic_chunking_text(text, max_chunk_size, overlap)

def semantic_chunking_text(text, max_chunk_size=1000, overlap=100):
    """Split plain text into semantic chunks."""
    # Get sentences
    try:
        sentences = nltk.sent_tokenize(text)
    except:
        # Fallback to simple splitting if NLTK fails
        sentences = re.split(r'(?<=[.!?])\s+', text)
    
    chunks = []
    current_chunk = ""
    current_chunk_sentences = []
    
    for sentence in sentences:
        # If adding this sentence would exceed max size, finalize current chunk
        if len(current_chunk) + len(sentence) > max_chunk_size and current_chunk:
            chunks.append({
                "text": current_chunk,
                "sentence_count": len(current_chunk_sentences),
                "position": len(chunks)
            })
            
            # Start new chunk with overlap
            overlap_text = ""
            overlap_sentences = []
            
            # Add sentences from the end of the previous chunk for overlap
            overlap_chars = 0
            for s in reversed(current_chunk_sentences):
                if overlap_chars + len(s) <= overlap:
                    overlap_sentences.insert(0, s)
                    overlap_chars += len(s)
                else:
                    break
                    
            overlap_text = " ".join(overlap_sentences)
            current_chunk = overlap_text + " " + sentence
            current_chunk_sentences = overlap_sentences + [sentence]
        else:
            # Add to current chunk
            if current_chunk:
                current_chunk += " " + sentence
            else:
                current_chunk = sentence
                
            current_chunk_sentences.append(sentence)
    
    # Add the last chunk if it's not empty
    if current_chunk:
        chunks.append({
            "text": current_chunk,
            "sentence_count": len(current_chunk_sentences),
            "position": len(chunks)
        })
    
    return chunks

def semantic_chunking_html(html_text, max_chunk_size=1000, overlap=100):
    """Split HTML into semantic chunks that respect HTML structure."""
    soup = BeautifulSoup(html_text, 'html.parser')
    
    # Extract headings and their content sections
    sections = []
    current_heading = "Introduction"
    current_content = []
    
    # Process all top-level elements
    for element in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'p', 'ul', 'ol', 'div']):
        if element.name.startswith('h'):
            # If we have content in the current section, add it to sections
            if current_content:
                section_text = " ".join([str(e) for e in current_content])
                sections.append({
                    "heading": current_heading,
                    "content": section_text
                })
            
            # Start a new section
            current_heading = element.get_text().strip()
            current_content = []
        else:
            # Add this element to the current section
            current_content.append(element)
    
    # Add the last section if it has content
    if current_content:
        section_text = " ".join([str(e) for e in current_content])
        sections.append({
            "heading": current_heading,
            "content": section_text
        })
    
    # Now chunk each section while preserving the heading as context
    chunks = []
    
    for i, section in enumerate(sections):
        heading = section["heading"]
        content = section["content"]
        
        # Skip very small sections or empty ones
        if len(content) < 50:
            continue
        
        # Remove HTML tags for text processing
        text_content = BeautifulSoup(content, 'html.parser').get_text()
        
        # Get chunks for this section
        section_chunks = semantic_chunking_text(text_content, max_chunk_size - len(heading) - 20, overlap)
        
        # Add heading context to each chunk
        for j, chunk in enumerate(section_chunks):
            chunks.append({
                "text": f"{heading}:\n{chunk['text']}",
                "sentence_count": chunk["sentence_count"],
                "position": len(chunks),
                "section": i,
                "section_position": j
            })
    
    # If no chunks were created, create one from the full text
    if not chunks:
        text_content = soup.get_text()
        return semantic_chunking_text(text_content, max_chunk_size, overlap)
    
    return chunks

# Function to process document text and extract relationships
def extract_document_structure(text, url):
    """
    Extract structured information from document text including:
    - Document type
    - Hierarchical structure (sections)
    - Key named entities
    - Parent-child relationships
    
    Returns a dict of structured information about the document.
    """
    # Determine document type
    doc_type = "unknown"
    if re.search(r'meeting|minutes|agenda', text.lower()):
        doc_type = "meeting_notes"
    elif re.search(r'report|analysis|overview', text.lower()):
        doc_type = "report"
    elif re.search(r'policy|procedure|guideline', text.lower()):
        doc_type = "policy"
    elif re.search(r'spec|specification|requirements', text.lower()):
        doc_type = "specification"
    
    # Extract sections and their hierarchy
    sections = []
    section_pattern = r'(?:^|\n)(#+|(?:Section|Chapter)\s+\d+)[ \t:]+(.+)(?:\n|$)'
    section_matches = re.finditer(section_pattern, text, re.MULTILINE)
    
    for match in section_matches:
        level_marker = match.group(1)
        title = match.group(2).strip()
        
        # Determine section level from the marker
        level = 1
        if level_marker.startswith('#'):
            level = len(level_marker)
        elif 'section' in level_marker.lower() or 'chapter' in level_marker.lower():
            level = 1
            
        sections.append({
            "title": title,
            "level": level
        })
    
    # Extract entities (people, projects, dates)
    entities = {
        "people": [],
        "projects": [],
        "dates": []
    }
    
    # Simple pattern matching for entities
    # People: capitalized names
    person_pattern = r'\b(?:[A-Z][a-z]+\s+){1,2}[A-Z][a-z]+\b'
    for match in re.finditer(person_pattern, text):
        name = match.group(0)
        if name not in entities["people"]:
            entities["people"].append(name)
    
    # Projects: capitalized phrases possibly with numbers
    project_pattern = r'\b(?:[A-Z][a-z]*[ \-]){1,3}(?:Project|Initiative|Program|Platform)\b'
    for match in re.finditer(project_pattern, text):
        project = match.group(0)
        if project not in entities["projects"]:
            entities["projects"].append(project)
    
    # Dates: common date formats
    date_pattern = r'\b(?:\d{1,2}[-/]\d{1,2}[-/]\d{2,4}|\d{4}[-/]\d{1,2}[-/]\d{1,2}|(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\.?\s+\d{1,2},?\s+\d{2,4})\b'
    for match in re.finditer(date_pattern, text):
        date = match.group(0)
        if date not in entities["dates"]:
            entities["dates"].append(date)
    
    # Build document structure
    doc_structure = {
        "url": url,
        "type": doc_type,
        "sections": sections,
        "entities": entities,
    }
    
    return doc_structure

# Function to get dataset
def import_csv(df, csv_file, max_rows=None):
    """
    Fetch content from Confluence or fallback to CSV
    """
    try:
        # Get authenticated Confluence client
        confluence = get_confluence_client()
        
        # Get confluence domain for building URLs
        confluence_domain = os.getenv('CONFLUENCE_DOMAIN')
        
        # Get all spaces with pagination
        all_spaces = []
        space_start = 0
        space_limit = 1000
        
        while True:
            spaces = confluence.get_all_spaces(
                start=space_start, 
                limit=space_limit, 
                expand='description.plain'
            )
            
            if not isinstance(spaces, dict) or 'results' not in spaces:
                break
                
            current_spaces = spaces['results']
            if not current_spaces:
                break
                
            all_spaces.extend(current_spaces)
            print(f"Found {len(current_spaces)} more spaces (total: {len(all_spaces)})")
            
            if len(current_spaces) < space_limit:
                break
                
            space_start += len(current_spaces)
        
        data = []
        id_counter = 1
        
        for space in all_spaces:
            # Skip archival or personal spaces if needed
            if space['type'] != 'global':
                continue
                
            print(f"\nProcessing space: {space['key']}")
            
            # Get pages from space with pagination
            page_start = 0
            while True:
                pages = confluence.get_all_pages_from_space(
                    space['key'], 
                    start=page_start,
                    limit=100,  # Can increase this if needed
                    status='current'
                )
                
                if not isinstance(pages, dict) or 'results' not in pages:
                    current_pages = pages if isinstance(pages, list) else []
                else:
                    current_pages = pages['results']
                
                if not current_pages:
                    break
                    
                print(f"Processing {len(current_pages)} pages from {space['key']} (starting at {page_start})")
                
                for page in current_pages:
                    try:
                        page_content = confluence.get_page_by_id(
                            page['id'], 
                            expand='body.storage'
                        )
                        content = page_content['body']['storage']['value']
                        
                        data.append({
                            'id': str(id_counter),
                            'tiny_link': f"{confluence_domain}/pages/viewpage.action?pageId={page['id']}",
                            'content': content,
                            'space_key': space['key']  # Added space key for tracking
                        })
                        id_counter += 1
                        
                        if max_rows and id_counter > max_rows:
                            break
                    except Exception as e:
                        print(f"Error processing page {page['id']} in space {space['key']}: {str(e)}")
                
                if max_rows and id_counter > max_rows:
                    break
                    
                # Move to next page of results
                page_start += len(current_pages)
                
                # If we got less than the limit, we've reached the end
                if len(current_pages) < 100:
                    break
            
            print(f"Completed space {space['key']} with {id_counter} total pages")
        
        # Convert to DataFrame
        new_df = pd.DataFrame(data)
        print("\nFinal statistics:")
        space_counts = new_df.groupby('space_key').size()
        print("\nPages per space:")
        for space_key, count in space_counts.items():
            print(f"  {space_key}: {count} pages")
            
        return new_df
        
    except Exception as e:
        print(f"Error in import_csv: {str(e)}")
        if csv_file and os.path.exists(csv_file):
            print(f"Falling back to CSV file: {csv_file}")
            return pd.read_csv(csv_file)
        raise

def clean_data_pinecone_schema(df):
    # Check if df is None
    if df is None:
        return "Error: No data was loaded from either Confluence or CSV"
    
    # Add detailed debugging of input dataframe
    print(f"Input DataFrame shape: {df.shape}")
    print(f"Input DataFrame columns: {df.columns.tolist()}")
    
    # Load spaces configuration from environment variable
    configured_spaces = os.getenv('SPACES', '').split(',')
    configured_spaces = [space.strip() for space in configured_spaces if space.strip()]
    print(f"Configured spaces from environment: {configured_spaces}")
    
    # Update to check for either 'tiny_link' or 'url' column
    if 'tiny_link' in df.columns:
        link_column = 'tiny_link'
        print(f"Using 'tiny_link' column for URLs")
    elif 'url' in df.columns:
        link_column = 'url'
        print(f"Using 'url' column for URLs")
    else:
        # If neither 'tiny_link' nor 'url' is present
        return "Error: CSV file is missing required link column: either 'tiny_link' or 'url' must be present"
    
    # Ensure necessary columns are present
    required_columns = {'id', 'content'}
    if not required_columns.issubset(df.columns):
        missing_columns = required_columns - set(df.columns)
        return f"Error: CSV file is missing required columns: {missing_columns}"
    
    # Check if space column exists
    space_column_exists = 'space' in df.columns
    if not space_column_exists:
        print("Warning: 'space' column not found. Will extract space keys from URLs or use configured spaces.")
    else:
        space_counts = df['space'].value_counts().to_dict()
        print(f"Found existing 'space' column with values: {space_counts}")
    
    # Filter out rows where 'content' is empty
    df_filtered = df[df['content'].notna() & (df['content'] != '')].copy()
    
    if df_filtered.empty:
        return "Error: No valid data found in the CSV file after filtering empty content."
    
    # Proceed with the function's main logic - operate on the copy
    df_filtered['id'] = df_filtered['id'].astype(str)
    
    # Rename the link column to 'source'
    df_filtered.rename(columns={link_column: 'source'}, inplace=True)
    
    # If space column doesn't exist, create it with default value
    if not space_column_exists:
        df_filtered['space'] = 'default-index'
    
    # Extract space key from URL whether space column exists or not
    # This ensures we always use the space key, not the space name
    if 'source' in df_filtered.columns:
        # Print sample URLs for debugging
        print("\nSample URLs for space key extraction:")
        for i, url in enumerate(df_filtered['source'].head(10).tolist()):
            print(f"  URL {i}: {url}")
        
        # Try to extract space from Confluence URL
        try:
            def extract_space_key_from_url(url):
                # First, convert to string if not already
                if not isinstance(url, str):
                    url = str(url)
                
                # Get configured spaces from environment
                configured_spaces = os.getenv('SPACES', '').split(',')
                configured_spaces = [space.strip() for space in configured_spaces if space.strip()]
                
                # Check for spaceKey parameter in URL
                space_key_match = re.search(r'[?&]spaceKey=([^&]+)', url)
                if space_key_match:
                    space_key = space_key_match.group(1)
                    return space_key
                
                # Check for spaces/SPACEKEY in URL
                spaces_match = re.search(r'/spaces/([^/]+)', url)
                if spaces_match:
                    space_key = spaces_match.group(1)
                    return space_key
                
                # If no space found, use default or first space
                if configured_spaces:
                    return configured_spaces[0]
                else:
                    return 'default-index'
            
            # Apply space key extraction
            df_filtered['space_key'] = df_filtered['source'].apply(extract_space_key_from_url)
            
            # If we have a separate space column, use extracted space_key as a backup
            if space_column_exists:
                df_filtered['space'] = df_filtered['space'].fillna(df_filtered['space_key'])
            else:
                df_filtered['space'] = df_filtered['space_key']
            
            # Clean up temporary column
            if 'space_key' in df_filtered.columns:
                del df_filtered['space_key']
                
            print("\nSpace assignment results:")
            space_counts = df_filtered['space'].value_counts().to_dict()
            for space, count in space_counts.items():
                print(f"  {space}: {count} documents")
                
        except Exception as e:
            print(f"Error during space key extraction: {str(e)}")
    
    # Process documents to extract structure and create semantic chunks
    print("\nProcessing documents with semantic chunking...")
    
    # Create a new DataFrame for chunked content
    chunk_rows = []
    id_base = 10000  # Base ID for chunks to avoid collisions
    
    for i, row in tqdm(df_filtered.iterrows(), total=len(df_filtered), desc="Chunking documents"):
        try:
            # Extract document structure
            doc_structure = extract_document_structure(row['content'], row['source'])
            
            # Get semantic chunks
            chunks = semantic_chunking(row['content'])
            
            # Create a row for each chunk
            for j, chunk in enumerate(chunks):
                # Create a unique ID for this chunk
                chunk_id = f"{row['id']}-{j}"
                
                # Create metadata for this chunk
                metadata = {
                    'source': row['source'],
                    'parent_id': row['id'],
                    'text': chunk['text'],
                    'doc_type': doc_structure['type'],
                    'position': chunk['position'],
                    'sentence_count': chunk.get('sentence_count', 0),
                    'space': row['space'],
                    'section': chunk.get('section', 0),
                    'section_position': chunk.get('section_position', 0),
                }
                
                # Add sections info if available
                if doc_structure['sections'] and len(doc_structure['sections']) > 0:
                    metadata['has_sections'] = True
                    if chunk.get('section', 0) < len(doc_structure['sections']):
                        section_info = doc_structure['sections'][chunk.get('section', 0)]
                        metadata['section_title'] = section_info.get('title', '')
                        metadata['section_level'] = section_info.get('level', 1)
                
                # Add entity information
                if doc_structure['entities']['people']:
                    metadata['people'] = ','.join(doc_structure['entities']['people'][:5])
                if doc_structure['entities']['projects']:
                    metadata['projects'] = ','.join(doc_structure['entities']['projects'][:5])
                if doc_structure['entities']['dates']:
                    metadata['dates'] = ','.join(doc_structure['entities']['dates'][:5])
                
                # Ensure metadata is JSON-serializable
                metadata_json = json.dumps(metadata)
                
                # Add row for this chunk
                chunk_rows.append({
                    'id': chunk_id,
                    'source': row['source'],
                    'content': chunk['text'],
                    'space': row['space'],
                    'metadata': metadata_json,
                    'parent_id': row['id']
                })
        except Exception as e:
            print(f"Error processing document {row['id']}: {str(e)}")
            # Add the whole document as one chunk if chunking fails
            chunk_rows.append({
                'id': f"{row['id']}-0",
                'source': row['source'],
                'content': row['content'],
                'space': row['space'],
                'metadata': json.dumps({
                    'source': row['source'],
                    'text': row['content'],
                    'parent_id': row['id'],
                    'space': row['space']
                }),
                'parent_id': row['id']
            })
    
    # Create a new DataFrame from chunks
    df_chunks = pd.DataFrame(chunk_rows)
    
    print(f"Original documents: {len(df_filtered)}, Chunked documents: {len(df_chunks)}")
    
    # Use the chunked documents for further processing
    df_filtered = df_chunks
    
    # Handle metadata (truncation if needed)
    # Define truncation function
    def truncate_text(text):
        """Truncate text to a maximum length with ellipsis."""
        max_text_length = 8000  # Maximum length for text field in metadata
        if isinstance(text, str) and len(text) > max_text_length:
            return text[:max_text_length-3] + "..."
        return text
    
    # Apply metadata processing
    df_filtered['metadata'] = df_filtered.apply(process_metadata, axis=1)
    
    # Final column check
    if 'id' in df_filtered.columns and 'source' in df_filtered.columns and 'content' in df_filtered.columns and 'metadata' in df_filtered.columns:
        print(f"Final DataFrame ready with {len(df_filtered)} rows")
        return df_filtered
    else:
        missing_cols = set(['id', 'source', 'content', 'metadata']) - set(df_filtered.columns)
        return f"Error: Final DataFrame is missing required columns: {missing_cols}"

def process_metadata(row):
    """Process metadata to ensure it's in a consistent format."""
    try:
        # First check if it's a string and convert to dict
        if isinstance(row['metadata'], str):
            try:
                metadata = ast.literal_eval(row['metadata'])
            except:
                # If parsing fails, create a basic metadata dict
                metadata = {
                    'source': row['source'],
                    'text': row['content']
                }
        else:
            # Already a dict
            metadata = row['metadata']
            
        # Ensure text is not too long
        if 'text' in metadata:
            metadata['text'] = truncate_text(metadata['text'])
            
        # Add space information to metadata
        metadata['space'] = row['space']
        
        try:
        try:\            # Convert back to string\            return json.dumps(metadata)\        except Exception as e:\            print(f"Error processing metadata for row {row["id"]}: {str(e)}")\            # Return a basic metadata if processing fails\            return json.dumps({\                "source": row["source"],\                "text": truncate_text(row["content"]),\                "space": row["space"]\            })
                'text': truncate_text(row['content']),
                'space': row['space']
            })
    except Exception as e:
        print(f"Error in process_metadata: {str(e)}")
        return json.dumps({
            'source': row.get('source', 'unknown'),
            'text': truncate_text(row.get('content', '')),
            'space': row.get('space', 'unknown')
        })

# Function to generate embeddings and add to DataFrame
def generate_embeddings_and_add_to_df(df, model_for_openai_embedding):
    # Ensure df is a DataFrame
    if isinstance(df, str):
        try:
            df = pd.read_csv(df)  # If df is a file path
        except Exception as e:
            print(f"Error reading CSV file: {e}")
            return None
            
    if df is None or not isinstance(df, pd.DataFrame):
        print("Error: Input is not a valid DataFrame")
        return None
        
    if 'metadata' not in df.columns:
        print("Error: DataFrame is missing 'metadata' column")
        return None
    
    # Store columns we want to preserve
    preserve_columns = ['id', 'metadata']
    if 'space' in df.columns:
        preserve_columns.append('space')
        print(f"Will preserve 'space' column with {df['space'].nunique()} unique values")
    
    print("Start: Generating embeddings and adding to DataFrame")
    
    df['values'] = None
    # OpenAI's text-embedding-ada-002 has a token limit of 8191
    # But we'll be conservative since token count estimation is not exact
    # 1 token is roughly 4 chars in English, so 8000 tokens ≈ 32,000 chars
    # We'll be more conservative and use 28,000 chars initially
    safe_token_limit = 7000  # Characters (approx 7000 tokens)
    max_retries = 3  # Maximum number of retries for each row
    
    # We'll track failed rows to report at the end
    failed_rows = []
    retry_success = 0
    truncated_rows = 0

    for index, row in tqdm(df.iterrows(), total=df.shape[0]):
        try:
            content = row['metadata']
            meta = json.loads(content)
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON for row {index}: {e}")
            failed_rows.append(index)
            continue  # Skip to the next iteration

        text = meta.get('text', '')
        if not text:
            print(f"Warning: Missing 'text' in metadata for row {index}. Skipping.")
            failed_rows.append(index)
            continue

        # Pre-emptively truncate text to avoid token limit errors
        original_length = len(text)
        if original_length > safe_token_limit * 4:  # 4 chars per token approximation
            text = text[:safe_token_limit * 4]  # Initial conservative truncation
            truncated_rows += 1
            # Only log truncation if it's significant 
            if original_length > 1.5 * safe_token_limit * 4:
                print(f"Warning: Text for row {index} was truncated from {original_length} to {len(text)} chars")
            
        # Try embedding with progressive truncation if needed
        success = False
        retry_count = 0
        current_text = text
        
        while not success and retry_count < max_retries:
            try:
                response = create_embeddings(current_text, model_for_openai_embedding)
                if response is not None:
                    df.at[index, 'values'] = response
                    success = True
                    break
                else:
                    print(f"Warning: Empty embedding response for row {index}")
            except Exception as e:
                error_msg = str(e)
                
                # Check if it's a token limit error
                if "maximum context length" in error_msg or "token" in error_msg:
                    # Calculate a more aggressive truncation
                    # Each retry, reduce by half until we're under the limit
                    current_length = len(current_text)
                    new_length = current_length // 2
                    
                    print(f"Retry {retry_count+1} for row {index}: Truncating from {current_length} to {new_length} chars")
                    current_text = current_text[:new_length]
                    
                    # If we're getting very short and still having issues, give up
                    if new_length < 1000 and retry_count > 0:
                        print(f"Text too short to retry for row {index}, giving up")
                        break
                else:
                    # If it's not a token limit error, no point retrying
                    print(f"Non-token error for row {index}: {error_msg}")
                    break
                    
            retry_count += 1
        
        if success and retry_count > 0:
            retry_success += 1
        
        if not success:
            print(f"Failed to generate embedding for row {index} after {retry_count} retries")
            failed_rows.append(index)

    # Print before drop
    print(f"DataFrame before dropping failed rows: {df.shape}")
    print(f"Columns before drop: {df.columns.tolist()}")

    # Remove rows with None values for 'values' column before returning
    original_count = df.shape[0]
    df = df.dropna(subset=['values'])
    final_count = df.shape[0]
    dropped_count = original_count - final_count
    
    # Ensure we preserved all important columns
    print(f"Columns after drop: {df.columns.tolist()}")
    for col in preserve_columns:
        if col not in df.columns:
            print(f"WARNING: {col} column was lost during processing!")
    
    # Debug space column if it exists
    if 'space' in df.columns:
        space_counts = df['space'].value_counts().to_dict()
        print(f"Space column values after processing: {space_counts}")
    
    # Print summary statistics
    print("\nEmbedding Generation Summary:")
    print(f"- Total rows processed: {original_count}")
    print(f"- Rows where text was truncated: {truncated_rows}")
    print(f"- Rows that succeeded after retry: {retry_success}")
    print(f"- Rows that failed completely: {len(failed_rows)}")
    print(f"- Success rate: {(final_count / original_count) * 100:.2f}%")
    
    if dropped_count > 0:
        print(f"Warning: {dropped_count} rows were dropped due to failed embedding generation")
    
    print("Done: Generating embeddings and adding to DataFrame")
    return df

